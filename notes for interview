## what I do with terraform is 
 * create S3 bucket where we store our statefile
 * create a dynamoDB to lock our statefile
 ##
 #### what is the difference between desire state and current state?

     ## issues encounter:  
     - map function ????? 

      #https://www.tinfoilcipher.co.uk/2021/05/05/terraform-applying-common-tags-to-resources-using-maps/

resource "aws_instance" "private_node" {
   count                       = 10
   availability_zone           = data.aws_availability_zones.available.names[0]
   ami                         = data.aws_ami.tinfoil_ubuntu.id
   instance_type               = "t2.micro"
   key_name                    = "tinfoilkey"
   subnet_id                   = local.subnet_tinfoil_private
   vpc_security_group_ids      = [local.sg_tinfoil_private]
   tags = merge(var.default_tags,{
       Name = "Instance-${var.environment_name}${count.index}"
       },
   )
#}
 - map function ????? 
 -
 -
   ## create the vpc  that can be use at multiple enviroment at ones.
  
  part 2 video  40:00 
  
   what do you need to launch vpc in terraform ?
    - provider region where to deploy
    -security group
    - the vpc itself
    -elastic ip  beacause if you need natgateway you will need to attach ilps to the natgateway.
    -next you creat the internet gateway 
    -subnet
    -then natgateway
    - bashost should be very secure because you can gain access throught bashost then db. 

why to use vpc?   (private vituel cloud) .....
# create subnet
when you create  vpc you have to create NAT GATEWAY in orde to have high availibility zone . at least 2 so if one NAT gatway goes down, the second will be up and you application should be up. 
# create internet gateway  : same like port in k8s. 
how do you securtly connect on you corporate network?   throught  vpn . so from your computer to vpn then corporate and aws. 

in order to avoir ip conflit , each subnet  should have hi own cidr_block

    consepte of share and own.
    share: 99% of time you will find share (vpc) meaen you can modifie and deploy vpc
    own mean you cant. it is own by x or y 


    ################### i have to create the env dev prod and qa so create the path localy to call my resource from the module.  I could put my code also in github
    and refenced so i just call them but i have been ask to create the path locally for some reason. 

    #### my manager ask me to create a vpc in a new availability zone  ohio and oregon because we had a new customer  , so i just have to use my other resources from dev
    modifier and launch in the new env prod.
      ---- i asked my manager what is the vpc  cidr_block?
      ----whart are the private subnet, public and availability_zone?


      I create module, then create resource and call the module from the resources. 

      i have to create 3 env  vpc .  dev prod ens qa   so i go in my module and copy vpc then paste in resource. en modifier.  

      Terraform acquires a state lock to protect the state from being written
â”‚ by multiple users at the same time.

# when your dynamo statefile is lock and you want to unlock, do it throught the command line  look for the cmd online 
#  command: force-unlock    terraform force-unlock (option) lock_ID

# one issue that I have been encounted was about mysql passwoord. the dev env could not pull the db from terraform the credential were previewsly store in 
# ansibe  and terraform could not deploy mysql. so I had to reconfigure the db and store the credential in  aws s3 bucket . 

##### parameter store  some compagnies use to store non sensentive data. 
# secret manager to store sensetire data.  credential. 

last seen   viseo part 4 db

deploy in multiple environment without expose your aws secret key. 

from git terminal

aws s3 ls --profile dev

aws s3 ls --profile prod

aws s3 ls --profile qa

#############################################################################################################

#### DB CREATION FROM TERRAFORM###########
-make sure you s3 bucket and dynamoDB table are up and running
-then lunch you bastion host / EC2 instance.
-then go to git bash and open ec2    do    sudo -i 
    sudo -i 

    # SECRET MANAGER 
    create the db rds password from terraform and input manualy the password in secret manager 
on parameter STORE. WE WILL CREATE THE password for db rds  WITHOUT SECRET VALUES AND WE WILL GO IN
 aws  secret manager INPUT MANUALY THE SECRET VALUE IN THERE . ( RIGHT WAY TO DO)

@@@@@@ proble encounte is when i was creating the rds password in tf, (secret manager) , i put the usename and password in my code then deployed
evrything was great for a month until my manager asked me to retrieve the password of the db and handle to him. i COULD NOT because i deployed
the code way back ago to AWS secret manager , but the statefile was local instead to be in aws statefile with and lock with dynamoBD.

@@@@ NOW create the DB      part 5   52:31

# for more security, dont expose you db public should be private

#lock the state file mean two engineer cant apply terraform apply at the same time.





part 5 1:17

PART 5 
#####PROBLEMES THAT I ENCOUNT IS WHEN I WAS CREATED MY PROSGRES DB, 
##I CREATE THE USERNAME AND PASSWORD OF MY RDS  DB . THE MISTAKE WAS WHEN I WERE REFEREING
# THE USERNAME AND PASSWORK IN THE MAIN.TF, i DID NOT SEE THE DIFFERENT
#"rds_password"  AND IN MY DATA SOURCE WAS rds-password
# SO WHEN I TERRAFORM PLAN I HAD THE ERROR AND TERRAFORM SHOW ME EXACTLY WHERE TO GO FIX

##################################################################################

stopped at 1:36  part 5


####mine### go paste on git bash   and type \l
#psql --host=alpha-cicd-db.cbggprguolhn.us-east-1.rds.amazonaws.com --port=5432 --username=coubis_admin --password --dbname=alpha



###PROBLEM THAT I INCOUNT IN THE PROJECT THAT I WORK IN MY COMPAGNT########
We were working on the project call Stomer that I have been assign the task to create the database and also create the record and route 53 using 
the domain name. so I create subdomain name and route 53 so that instead to use database endpoint, to login in the database,  we used the sub domain 
to login .  That was the first time that we used that domain name. when my manager handle to me, I was able to create a database and login 
from the info and everything works perfectly. But when I  created the record route 53 and tried to connected, it could not able to connected. 
That is where I started trouble shooting. First of all I ping the domain, and I could not find a domain.  cannot respond to ping. 
I did more troubleshooting in AWS and I found out my manager  did not confirm the email that AWS sent when you create the new domain after 15 days 
so my manager reached out to AWS because we had AWS support and he request the new email that they send it back to him and he confoirmed  and everything was solve 
at that point.

 I went check in aws the domaine if the domain was register . I found out it was not. Registered domains > betacoubis.link


 why you guys used sub domain?  because we had new team so we had to create sub domain from the main domain. 

            create multiple env in less time or on demand  
###  create tamplete ( TF CODE MODULE) and that module should be able to create our infractructure in one env.and push in one repository(Github) 
# then we can trigger TF Runs code via jenkins,or terraform to create multiple environement. 
# now because each env are not the same capacity, we create the tamplete for each env, and we create the file for each env where i gonna put all the values and when we run tf init,plan , terraform will go and look the tamplate to the repository and create each different env depending on the values.

# 

 # part 6 is for aws secret manager count with .... i need to redo with Tia



 # part 7  1:20

 # trick to cd into folder   video part 7  1:17 


coment out  /* andfettyt   */

## how do you destroy and resource in terraform ?   you can hiden, removed, comment out and do terraform apply

for security  we set up to apply terraform only on master.
s3 tia video part 2  2:27:00   tell me the project that you work on so far. 




################################################################################
### project that i have been working on it ###
My team and I were working on the project call STOMER where i have been assigned to create the entire VPC and for 2 availablility Zone .
those 2 availability zone should host each of them 
2 private subnet, for  1 for EC2 AND 1 for DATABASE. prosgress
1 publuc subnet for the NAT gateway  for high availablility
1 Elastic IP address 
and both NAT gateway should convert to the INTERNALGATEWAY. the entry point


so for that project I had to set up my VS CODE link with new aws account that have been assiged to me in order to deploy my code in that account. so i configure   aws configure   then  aws s3 ls  to verifier that my aws account is link with my vs code. 

also i had to set uo my vs code with terraform since i had to use terraform to build my code and deploy in AWS account. 

when all are set up,  all the details have been handle to me such as :
   
   variable common tag
    "AssetID"       = "2560"
    "AssetName"     = "Insfrastructure"
    "Environment"   = "dev"
    "Project"       = "alpha"
    "Name"          = bucket name 
    "CreateBy"      = "Terraform"
    "cloudProvider" = "aws"
  and others details that i should need to build my vpc and push in those availability  zone. 

then I create the module tamplete that i used to create others resourses such as : 
1  bucket and dynamodb   that will allow  to store and lock statefile .
  in resource  and i referet to modules vpc and backend
2  create dev env   and deploy vpc in the s3 bucket plus dynamodb lock file
3  create qa env    and deploy vpc in the s3 bucket plus dynamodb lock file
4  create prod env   and deploy vpc in the s3 bucket plus dynamodb lock file

then I have  create a new VPC for the new insfrastructure  with custome name and others detals  in order to enforce security. and for that i have to build 
1    set up the provider
vpc itself, 
2 vpc Backend and dynamodb  that i refert to the main backend module
3 vpc Provider Block 
   
4 elastic ip address   (2)  for both availablility zone  for the static because we dont want that to change. 
5 internet_gateway   (1)  like main entry port / it is where traffic is come in and out throught the internet gatway
6 subnets and for each availablility zone i create 2 provider subnet for DB and EC2 instances , 1 public sub for NAT gateway  remenber by default it is public until we attach on db(private) or nat gatway(pubic)
7 nat_gateway    (2 )   because i create those db and EC2 in the  private subnet , i create the NATgatway in the public subnet and i issigned to the Elastic IP address
8  so if we need something in the private subnet, it has to go throught the PUBLUC SUBNET THEN THE NAT GATWAY  THEN THE IP THEN THE INTERNET GATWAY
9 route_table_association  
10 bastion    pretty much allow thing to work like ec2, git , ... so , just keep port 22 open. ssh 22 
   bastion host is in the public subnet/ and  should have only port 22 open for more security/ 

# auto scaling explanation in two availibility zone  video part 2 2:00
# how do you handle different version of terraform?  video part 1  vert la fin.
 - if terraform version is under 1, evryone should use the same version so upgrate to the same version  
 if the version are under 1 and 2 engineerer do terraform apply, the statefile will lock because of version conflict

 to fix that, i  build the Dockerfile with the specific version that we used and push the file in DockerHub and i enforced the entile team to use that specific version  

 so if you want to run tarraform, you will be force to mount your directory inside the container and use that container to run terraform apply. 

- but if the version is over 1, we should not worry about the version it will automatic #### BACKWORK COMPATIBLE#####   terraform solve that issue. 


@@@@@ terraform part 2  video 20:00    name the tags  and ref the tag 24:00



stoped at 
part 2     2 : 27 


create the secret manager video part 4 2:30
create aws parrameter store  video part 4  4 :49 
pass word management .
dont create the database in tf, and MANUALYset up pass word in aws. 
 inconvenient is when you will change the code on tf, and do terraform apply, with a new password, terraform will considere the old pass word and 
  if dont remenber you will be stuck. 

    better way is the use PARAMETER STORE (cheap) for nom sensetive data  or secrete manager (expensive) for sentitive data. 


########### puch  to github video part 4    3:00

#######################################################################################################

### video 5 create database 

#### DB CREATION FROM TERRAFORM###########
-make sure you s3 bucket and dynamoDB table are up and running
-then lunch you bastion host / EC2 instance.
-then go to git bash and open ec2    do    sudo -i 
    sudo -i 

    # SECRET MANAGER 
    create the db rds password from terraform and input manualy the password in secret manager 
on parameter STORE. WE WILL CREATE THE password for db rds  WITHOUT SECRET VALUES AND WE WILL GO IN
 aws  secret manager INPUT MANUALY THE SECRET VALUE IN THERE . ( RIGHT WAY TO DO)

@@@@@@ proble encounte is when i was creating the rds password in tf, (secret manager) , i put the usename and password in my code then deployed
evrything was great for a month until my manager asked me to retrieve the password of the db and handle to him. i COULD NOT because i deployed
the code way back ago to AWS secret manager , but the statefile was local instead to be in aws statefile with and lock with dynamoBD.

@@@@ NOW create the DB      part 5   52:31

# for more security, dont expose you db public should be private

#lock the state file mean two engineer cant apply terraform apply at the same time.


part 5 1:17

PART 5 
#####PROBLEMES THAT I ENCOUNT IS WHEN I WAS CREATED MY PROSGRES DB, 
##I CREATE THE USERNAME AND PASSWORD OF MY RDS  DB . THE MISTAKE WAS WHEN I WERE REFEREING
# THE USERNAME AND PASSWORK IN THE MAIN.TF, i DID NOT SEE THE DIFFERENT
#"rds_password"  AND IN MY DATA SOURCE WAS rds-password
# SO WHEN I TERRAFORM PLAN I HAD THE ERROR AND TERRAFORM SHOW ME EXACTLY WHERE TO GO FIX



part 5  8:27  create the database 
# so we create the secrete with no  values and we manuelly input the value in aws consol so  
we create data base and we store credential in secret manager .   when we rotate the password in secret manager after 3 months, for eg, 


# "create-secret-with-no-value "   40:00   part 5 
# create first "create-secret-with-no-value " then go input usename and password manuelly in AWS then now go back on terraform add data.tf and paste name from usename and passeword 
# 


TO CREATE DATABASE, WE NEED VPC, SUBNET SECURITY GROUP PART 5 55:00 

stop at part 5  2;20    database.  ready to apply but i didnot do yet. 




stop on part 7   1:42 

create alb 
we need
1 certificate ID ( aws certificate manager (acm))  and we attached on the load balancer not on the ec2. 
2 elb
  - create alb cname -record.tf (id zone go get in route 53)
  -


EKS DEPLOYEMENT

Blue green ..... 


######################terraform statefile##############################

when you type terraform init , 

####################################################################################
link your vs-code with git and clone a repository ito you vs code. 

video Mach 29.2022 session 3 April class recording terraform state file. 
video 07:54 



##############################################################

Following all stages to deploy apps on the browser using Terraform and AWS

1 Create an s3 bucket for Terraform state file

  CREATE YOUR BUCKET THROGHT TERRAFORM AND DEPLOY TO AWS ACCOUNT.
  - Bucket will be starage for all your STATEFILE, AND MODULES  

2 dynamoDb
   Create a DynamoDB table to lock  the state file 
  - to protect your state file to be lock, the dynamoDB will be implement. 
3 VPC
   Create a VPC with the following:
    Public Subnets for load balancers, bastion host servers, and EKS control plane
    Create private Subnets for the Database 
    Create private Subnets for Kubernetes nodes groups
4 BASTION HOST
   - Create a bastion host server in the public subnets
   -Create an EC2 in the private subnet and test if we can connect to it through through 
   the bastion host in the public subnet

5 Create databases

6 Create an ALB (application load balancer)
 - Create an ALB security   (aws-parametre-store )

- Create an ALB in the public Subnets

- Create ALB target groups

- Register EC2 as target groups

- Route HTTP traffic to HTTPS

- Create an SSL certificate and attach it to the load balancer

- Create the ALB CNAME record in Route 53

- Host a website in all EC2 behind the load balancer

- Access the website through HTTPS on the browser


7-  Create EKS

Discuss Kubernetes cluster creator

Create a Kubernetes control plane in the private subnets

Create Kubernetes node groups in the private Subnets and attach them to the control plane

Connect to Kubernetes nodes in the private Subnets from the bastion host

Implement Kubernetes RBAC (role-based access control)

  Developer access

  Admin Access 

  Readonly access

  Cluster role and cluster role binding

  Role and role binding

Upgrade Kubernetes cluster from one version to another and flip the node with a blue-green nodes deployment strategy

Install Kubernetes cluster auto-scaler and implement Kubernetes nodes autoscaling

Decommission Kubernetes nodes from the cluster

7  
   Upgrade Terraform from from 0.12.X to 1.0.X



   ########################################################################################################
   WELLCOME TO DECKER      1:00:00  sep  getting started with Docker

    back in the day we had multiple computer(windonw 8,7,10,11,.. mean more hardware) and application on server

    now Hypervisor has just 1 hardware, giant server. 
        

-----> type hypervisor 2 
hardware --- Host OS -- Hypervisor -- then vm 
we install HOST OS on top of hardware ---we install  hypervisor on top of HOST OS --- we install our VM on top of hypervisor (for testing env)
type hypervisor 2   pro ------> you can lunch multiple vituel machenes   windonw , unbuntu, centos, red add, jenskins..... 
                    cons ------> Hypervisor use more memory ram .   
                                  cant open 2 same port on the same server        that is   why create Docker  d ou contener --> dont have carner, host os, 
     now in the contener we can install                              
pro of contener ---->  less ram consoming
docker engine share the same ios with server Host os ,  contener dont have ios. 
light weight vm,  or conteners  1:43:00 


                ---->  parkarging code from developer (no more dispute with developer and devops)
-----> type 1 hypervisor 
hardware -- Hypervisor -- then vm     ( NO Host OS)

#######################################################################################
DOCKER installation

cat /etc/*release
for aws linux machine   
sudo -i 
yum update

# install docker engine   follow the link 
https://docs.docker.com/engine/install/ubuntu/

# verifier docker    and make sure docker is running
# systemctl status docker   (deamond if you stop this you stop the contenaire.) 
S5 FEBUARY VIDEO GETTING STARTED WITH DOCKER AWS hand on (Docker volume portForwarding)
1:10
--- ubuntu@ip-172-31-92-75:~$ docker --version
Docker version 24.0.5, build ced0996
ubuntu@ip-172-31-92-75:~$ systemctl status docker                   # i create image 
       now we are redeay to type docker command
       docker images for eg. 
############################################################################################

https://hub.docker.com/  
devopseasylearning
DevOps2021@

-- repositories   folder where you push all your images 
-- Docker naming convention                                          # i push in DockerHub in the specific repository
<username>/repository  name:tag
   devopseasylearning / s3-project01-espresso-shop-review:v1.0.0

example: 
------ docker pull centos:7
    check if image exist on your git   docker images |grep centos:7
    now type docker pull centos:7 

    now if i want to use this image to create contener,Itype :
    ###### docker run --rm --name s3coubis_centos -it eeb6ee3f44bd bash ########
    ####### docker run -d --name s3coubis-test httpd:latest bash #######
    docker run : create contener
    -- rm     :  mean destroy the contener when done 
     -name s3coubis_centos  :   assign name on you contener

####  docker run -itd --name s3coubis_centos centos:latest bash######
     -itd  : mean run the contener background
     check running at the backgroud 
     # docker ps |grep s3coubis_centos

                             # lognin in the contener running
      docker exec -it s3coubis_centos bash   you will be lock in that contenaire
      docker ps -a |grep s3coubis-test
      docker ps -a     liste ID (CONTENAIRES PORTS NAMES)
     
# docker images ------> to check all images in the contener
# check  running contener------->  
# stop the contener--------> # Using container ID
# see the contenaire exited -----> docker ps -a
docker stop <container_id>

# Using container name
docker stop <container_name>

# Using image ID
docker rmi <image_id>

# Stop the container
docker stop 0b44d6bf811e  (contenaire id)       # stop

# Remove the container                         # remove contenaire
docker rm 0b44d6bf811e
# remove image
docker rmi --force centos:latest


# Using image tag (repository:tag)
docker rmi <repository>:<tag>


# docker rm  (ID image)-------> to remove image
 

 yes we can run a commad without logni in the contenair   47:30 

docker exec -it s3coubis-test cat /etc/passwd
#  docker run -it --name s3coubis-test httpd:latest bash     create and login directly in the contenair?
#  docker run -d -P --name s3coubis-redis redis:latest   # -P CAPITAL -P  is free port forwarding  docker choise the port between 3000--65535
  then check   #  docker ps |grep s3coubis-redis
  -----> check if the port is used by someone else  # docker ps -a |grep (your choose port) 
  ------> bindin port forddd  # docker run -d -p 1650:80 --name s3coubis httpd:latest        low case -p   
    binding port forwarding   that is the use out there             choise you port 0-65000   because we dont want port to change . 

    1 docker ps -a |grep  rafy-test    2:00:00 docker handons 
    2 docker run --name rafy-test -d -p 750:80 httpd 
    output
    ---e1d6e26ee66f   httpd  "httpd-foreground"      0.0.0.0:750->80/tcp, :::750->80/tcp     rafy-test
    3 docker exec -it (contenaire name) bash
    4 cd /usr/local/apache2/htdocs/      # for httpd 
    5 ls   
    6 cat index.html
    check the operator system  # cat /etc/*release    then    apt updete  
     apt updete
     apt install vim -y
     apt install wget -y
     apt install unzip -y
    7 ls  then  vim index.html   :wq!
    after paste and check on the browser. you good to go 
    8 remove the index.html   
      rm -rf *
      ls      you should see emty
  
  9 now tia has web store in his bucker  and use wget to donwload it 
    https://linux-devops-course.s3.amazonaws.com/articles.zip

  go to your terminal and paste 
    wget https://linux-devops-course.s3.amazonaws.com/articles.zip
  ls     you will see   articles.zip 
  then go to terminal     
    unzip articles.zip 
    rm -rf articles.zip

    ls articles         you should have   Dockerfile code
    mkdir article 
    cp -r articles/code/*article   # copie evrything in articles and paste in article
    ls article 
    then go paste on the browser   
    server2.anomicatech.com:750/article

    or you can host the second on 
  
  mkdir covid
  cd covid   then ls      should see empty

then paste this in you contenaire 2:43:35  docker handons 3 

wget https://linux-devops-course.s3.amazonaws.com/WEB+SIDE+HTML/covid19.zip  
unzip covid19.zip 
cp -R covid19/* . 
rm -rf covid19.zip 
rm -rf covid19


--------------------------------------------------
wget https://linux-devops-course.s3.amazonaws.com/WEB+SIDE+HTML/static-website-example.zip 
unzip static-website-example.zip 
cp -R static-website-example/* . 
rm -rf static-website-example.zip 
rm -rf static-website-example
 ############################################################################################
 Docker interview contenaire create proccedure

 1- logni in my command line
 2- check if I have the actuel contenaire running or if someone using the same contenaire  # docker images
 3 go to Docker hub and grab the tag of the contenaire i want to create
 4 paste in my cmd line
 5 run the cmd # docker run -it --name s3coubis_centos centos:latest bash
 6i can check with # docker ps or docker images or docker ps -a 

How do we get access application outside the contenaire?
      docker 
-----> docker run -d -P --name s3coubis-redis redis:latest

########################################################################################
################# go on confluence s4session s4 class notes  s4 docker  s4 dockerfile ########

docker file final part 
--- we dont use docke commit to puch our image    03:00   docker file final part  video s4 
we use docker file , install all our parkage then we expose our application. 

what instruction have you use in your docker file?

FROM----> 
ADD----->  instruction can copy from local and download something from  remote location to your images.
COPY---->  ONLY COPY FROM YOUR LOCAL TO YOUR IMAGE   44:33   DOCKER S4 Hands on Docker file part 1
CMD----->  use to start to proccess  EG:  #you can overrerider (change)
ENTRYPOINT----> use to start to proccess   # you cant override you have to specifier emtrypoint before put any command you want
# docker run --rm -it --entrypoint /bin/bash test-tia
ENV---->    set environement 
EXPOSE ----> 
MAINTAINER----> LIKE THE ONWER
RUN----->   to run the instruction
USER-----> 
VOLUME------> directory where you can store you data
workdir----->  
ARG-------> 

2 ways to create images,  23:10
---> from running contenaires
---->  from Dockefile   (standar way to create image use by company)
---------------------------------------------------------------------------------
create the image 
--- touch Dockefile  
--- vim Dockerfile   and paste this 

FROM httpd
RUN apt update
RUN apt install vim -y
RUN apt install wget -y
RUN apt install unzip -y

### now build and create image
---- docker build -t coubis-test .
 -t means assig the tag on my image 
### now run the image
---- docker run --rm -it {image} bash     34:02 
        now you are inside the contenaire  
        check  wget, vim and unzip 34:40 
------exit the contenaire
### let say my Dockerfile is not in my courent directory, {.}   46:20
cp Dockerfile /tmp/
docker build -t coubis-testt /tmp/

ls  cat 
 then 
 docker build -t s3coubis-test .
  -t  assign the tag (how you want to call your image)
  .  means build here in my current directory

    in case i want to pull my image from /tmp/
  eg : cp Dockerfile /tmp/
docker build -it s3coubiss /tmp/



  lets run and log in the contenaire
  now check if wget, vim, and unzip are installed the run

docker run --rm -it --name coubis bash

--rm   remove contenaire when i am done 
-- it login in the contenaire

or build this   same 

FROM httpd
RUN apt update && apt install vim -y && apt install wget -y && apt install unzip -y



   emty the preview Dockerfile and create a new one
     >Dockerfile 
     then 
     vim Dockerfile 


FROM httpd
RUN apt update && \
    apt install vim -y && \
    apt install wget -y && \
    apt install unzip -y

RUN useradd coubis && \
    mkdir -p /tmp/test-docker && \
    cd /tmp/test-docker && \
    echo "We are learning" >> coubis.txt

------docker build -t eva .
then 
-----docker run --rm -it eva:latest bash
----cd /tmp/
----ls
----cd /test-docker/
----ls
----cat coubis.txt
----cat /etc/passwd |grep coubis
----
                                           examp2
FROM httpd

RUN apt update && \
    apt install vim -y && \
    apt install wget -y && \
    apt install unzip -y

# RUN cd /usr/local/apache2/htdocs/ 
WORKDIR /usr/local/apache2/htdocs/ 

# RUN rm -rf index.html

RUN mkdir covid && \
    cd covid && \
    wget https://linux-devops-course.s3.amazonaws.com/WEB+SIDE+HTML/covid19.zip  && \
    unzip covid19.zip && \
    cp -R covid19/* . && \
    rm -rf covid19.zip && \
    rm -rf covid19 


RUN mkdir static && \
    cd static && \
    wget https://linux-devops-course.s3.amazonaws.com/WEB+SIDE+HTML/static-website-example.zip && \
    unzip static-website-example.zip && \
    cp -R static-website-example/* . && \
    rm -rf static-website-example.zip && \
    rm -rf static-website-example 

------docker build -t harvey .
------docker run --rm -it harvey:latest bash
    ls  
    pwd   ( WORKDIR  /usr/local/apache2/htdocs/ )

-----
if you have issue, you can troubleshooting like this  
----docker run --rm -it eva:latest bash   1:01:51  final docker file video 

I stopped on break.

----------------------------------------------------------------------------------------------
                        video hands on Docker final part and getting started  1:45:36 
now build this dockerfile again  with expose 80 and CMD 


FROM httpd
RUN apt update && \
    apt install vim -y && \
    apt install wget -y && \
    apt install unzip -y

# RUN cd /usr/local/apache2/htdocs/ 
WORKDIR /usr/local/apache2/htdocs/ 

# RUN rm -rf index.html

RUN mkdir covid && \
    cd covid && \
    wget https://linux-devops-course.s3.amazonaws.com/WEB+SIDE+HTML/covid19.zip  && \
    unzip covid19.zip && \
    cp -R covid19/* . && \
    rm -rf covid19.zip && \
    rm -rf covid19 


RUN mkdir static && \
    cd static && \
    wget https://linux-devops-course.s3.amazonaws.com/WEB+SIDE+HTML/static-website-example.zip && \
    unzip static-website-example.zip && \
    cp -R static-website-example/* . && \
    rm -rf static-website-example.zip && \
    rm -rf static-website-example 

EXPOSE 80
# to start the web ?????
CMD ["apachectl", "-D",  "FOREGROUND"]


>Dockerfile
vim Dockerfile and paste  from 745 to 776

then do 
----docker build -t harvey . 
----docker run --rm -it harvey:latest bash
now exit to the contenaire and do 
---- docker run --name httpd-coubis-test1 -d -p 875:80 harvey:latest
-d means run background
-p to expose  (80)
check if the port 875 is used 
==================================================
another example

stop the preview contenaire
remove the contenaire 

FROM httpd

RUN apt update && apt install vim -y && apt install wget -y && apt install unzip -y


FROM httpd                                                  # rm -rf *    remove evrething 

RUN apt update && \
    apt install vim -y && \
    apt install wget -y && \
    apt install unzip -y

RUN useradd coubis && \
    mkdir -p /tmp/test-docker && \
    cd /tmp/test-docker && \
    echo "We are learning" >> coubis.txt

---------------------------------------------
FROM httpd

RUN apt update && \
    apt install vim -y && \
    apt install wget -y && \
    apt install unzip -y

# RUN cd /usr/local/apache2/htdocs/ 
WORKDIR /usr/local/apache2/htdocs/ 

# RUN rm -rf index.html

RUN mkdir covid && \
    cd covid && \
    wget https://linux-devops-course.s3.amazonaws.com/WEB+SIDE+HTML/covid19.zip  && \
    unzip covid19.zip && \
    cp -R covid19/* . && \
    rm -rf covid19.zip && \
    rm -rf covid19 


RUN mkdir static && \
    cd static && \
    wget https://linux-devops-course.s3.amazonaws.com/WEB+SIDE+HTML/static-website-example.zip && \
    unzip static-website-example.zip && \
    cp -R static-website-example/* . && \
    rm -rf static-website-example.zip && \
    rm -rf static-website-example 

                                        # 2:10:00
 RUN mkdir article && \                                        
     cd article && \
     wget https://linux-devops-course.s3.amazonaws.com/articles.zip && \
     unzip articles.zip && \
     rm -rf articles.zip && \
     cp -r articles/* ../article && \
     rm -rf articles

#RUN wget https://linux-devops-course.s3.amazonaws.com/articles.zip && \
#    unzip articles.zip && \
#    rm -rf articles.zip && \
#    cp -r articles/code . && \
#    rm -rf articles && \
#    mv code article
#
EXPOSE 80
# to start the web ?????
CMD ["apachectl", "-D",  "FOREGROUND"]

-----docker build -t harvey .
-----docker run --rm -it harvey:latest bash
exit in the contenaire then apply the below cmd

-----docker run --name httpd-coubis-test2 -d -p 850:80 harvey:latest

                                     let push this image on dockerHub  23:30

before you push on you rep., make sure you image follow the nameing convention like   ## devopseasylearning/s3coubis

let make this way :   devopseasylearning/s3coubis:httpd-coubis-test2

and for that we gonna rename this image or tag :     harvey:latest    23:30

------docker tag harvey:latest devopseasylearning/s3coubis:httpd-coubis-test2
output   is  devopseasylearning/s3coubis:httpd-coubis-test2

now go type docker images  on your terminal you will se the right image name that follow the name convention

now we can push to dockerhub

------ docker login       devopseasylearning
-------password DovOps2021@
-------docker push devopseasylearning/s3coubis:httpd-coubis-test2

 then go check on your dockehub repository.  

 git status   
 git add -A
 git commit -m "docker"


 #         VIDEO S4        Hands on Docker file part 1    

27:14  docker file name change issue    put  -f 
29:11  add and copy   instruction  practice. 

eg: ## COPY
donwload the code here 
 wget https://linux-devops-course.s3.amazonaws.com/WEB+SIDE+HTML/covid19.zip
 ---> unzip covid19.zip
 ---> rm -rf covid19.zip
 ---> ls 
 change location with WORKDIR   33:00 
---Dockerfile---
 ## COPY
FROM httpd
RUN apt update
# WORKDIR /usr/local/apache2/htdocs/ 
COPY covid19 /usr/local/apache2/htdocs/ 
EXPOSE 80
CMD ["apachectl", "-D",  "FOREGROUND"]
------ docker build -t coubis-tests .
------docker run --rm -it coubis-tests bash
now cd in /usr/local/apache2/htdocs/ 
----> cd /usr/local/apache2/htdocs/  38:07 
----> ls

##ADD
now use the same dockerfile and change 
## ADD    40:00 
FROM httpd
RUN apt update
WORKDIR /usr/local/apache2/htdocs/ 
RUN mkdir coubis
ADD covid19 ./coubis
EXPOSE 80
CMD ["apachectl", "-D",  "FOREGROUND"]

----- docker build -t coubis-testss .
-----docker run --rm -it coubis-testss bash
-----docker run --name coubis-test1 -p 8081:80 -d coubis-testss

SO:
ADD instruction can copy from local and download something from  remote location to your images.
COPY instruction can only copy from local to  the image


## COPY
FROM httpd
RUN apt update
# WORKDIR /usr/local/apache2/htdocs/ 
COPY covid19 /usr/local/apache2/htdocs/ 
EXPOSE 80
CMD ["apachectl", "-D",  "FOREGROUND"]


## ADD
FROM httpd
RUN apt update
WORKDIR /usr/local/apache2/htdocs/ 
RUN mkdir tia
ADD covid19 ./tia
EXPOSE 80
CMD ["apachectl", "-D",  "FOREGROUND"]

## ADD
FROM httpd
RUN apt update
WORKDIR /usr/local/apache2/htdocs/ 
ADD https://linux-devops-course.s3.amazonaws.com/articles.zip .
EXPOSE 80
CMD ["apachectl", "-D",  "FOREGROUND"]

## LABEL, USER   58:40  

FROM ubuntu
RUN apt update

LABEL "website.name"="geeksforgeeks website"
LABEL "website.tutorial-name"="docker"
LABEL "Date"="10-08-2020"
LABEL "Create"="Tia Tia"
VOLUME /data
USER root

## ARG, ENV , VOLUME    59:46   
ARG IS A VARIABLE..
eva-image

FROM ubuntu
ARG user=jenkins   # is you want create password 
ARG group=jenkins
ARG uid=1300
ARG gid=1300
ARG http_port=8080
ARG JENKINS_HOME=/var/jenkins_home

ENV DB_PASSWORD password
ENV DB_USER admin

RUN useradd $DB_USER

RUN mkdir -p $JENKINS_HOME \
    && chown ${uid}:${gid} $JENKINS_HOME \
    && groupadd -g ${gid} ${group} \
    && useradd -d "$JENKINS_HOME" -u ${uid} -g ${gid} -s /bin/bash ${user}

VOLUME $JENKINS_HOME
USER ${user}
EXPOSE ${http_port}

 ----docker build -t eva-image .
 ----docker run --rm -it eva-image bash
 ---id admin
 ---env
 ---id jenkins
 ---

#-----------# diferent  CMD and  EMTRYPOINT ---------##########

# CMD    you can overrerider (change)
FROM ubuntu
RUN apt-get update -y
CMD ["/bin/bash", "-c", "uname -a"]

----->docker build -t eva-imagecmd .
----->docker run --rm -it eva-imagecmd bash  # it will overrite means that change 2:00 s4 hand on dockerfile part 1
----->docker run --rm -it eva-imagecmd 
      #output
# Linux 61ec7b9287d0 5.15.0-1040-aws #45~20.04.1-Ubuntu SMP Tue Jul 11 19:08:13 UTC 
# 2023 x86_64 x86_64 x86_64 GNU/Linux
-----> uname -a       you will have the same output


FROM httpd

RUN apt update && \ 
    apt install vim -y && \
    apt install wget -y && \
    apt install unzip -y

# RUN cd /usr/local/apache2/htdocs/ 
WORKDIR /usr/local/apache2/htdocs/ 

RUN mkdir static && \
    cd static && \
    wget https://linux-devops-course.s3.amazonaws.com/WEB+SIDE+HTML/static-website-example.zip && \
    unzip static-website-example.zip && \
    cp -R static-website-example/* . && \
    rm -rf static-website-example.zip && \
    rm -rf static-website-example 

EXPOSE 80
CMD ["apachectl", "-D",  "FOREGROUND"]


## ENTRYPOINT    you cant override you have to specifier emtrypoint before put any command you want
FROM httpd

RUN apt update && \
    apt install vim -y && \
    apt install wget -y && \
    apt install unzip -y

# RUN cd /usr/local/apache2/htdocs/ 
#WORKDIR /usr/local/apache2/htdocs/ 

RUN mkdir static && \
    cd static && \
    wget https://linux-devops-course.s3.amazonaws.com/WEB+SIDE+HTML/static-website-example.zip && \
    unzip static-website-example.zip && \
    cp -R static-website-example/* . && \
    rm -rf static-website-example.zip && \
    rm -rf static-website-example 

EXPOSE 80
ENTRYPOINT ["apachectl", "-D",  "FOREGROUND"]


FROM ubuntu
RUN apt-get update -y
ENTRYPOINT ["/bin/bash", "-c", "uname -a"]

# docker run --rm -it --entrypoint /bin/bash test-tia
 CMD AND EMTRYPOINT  ARE using to start the deamon but 
 CMD 
 ENTRYPOINT 
 CMD----->  use to start to proccess  EG:  #you can overrerider (change)
ENTRYPOINT----> use to start to proccess   # you cant override you have to specifier emtrypoint before put any command you want
# docker run --rm -it --entrypoint /bin/bash test-tia
# docker run --rm -it --entrypoint /bin/bash evaa

##############################################################################
VOLUME AND PORT FOR 
S5 FEBUARY VIDEO GETTING STARTED WITH DOCKER AWS hand on (Docker volume portForwarding) 
37:40 explenaition 
43:40 practice 
1:00:00 around 